{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOALS OF THE NOTEBOOK\n",
    "\n",
    "In this repo I want to try and create a relational map of a movie database based on the information parsed from the database. First dataset we will use is a more limited set. However, later we will go through a Netflix dataset that we have generated. The end goal should be two fully fleshed out dendrograms. This will largely be demonstrating the python data analysis, statistics, and machine learning tools and technology I have been self learning.\n",
    "\n",
    "The datasets we will be using are a mash of sets found from Kaggle, Github, and Datacamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(5)\n",
    "\n",
    "# Read in IMDb and Wikipedia movie data (both in same file)\n",
    "movies_df = pd.read_csv(\"datasets/movies.csv\")\n",
    "\n",
    "print(\"Number of movies loaded: %s \" % (len(movies_df)))\n",
    "\n",
    "# Save a copy for testing purposes\n",
    "test_df = movies_df.copy()\n",
    "\n",
    "# Test got full length of movies data set\n",
    "def test_movies():\n",
    "    assert len(test_df) == 100\n",
    "\n",
    "# Test got all columns of movies data set\n",
    "def test_shape():\n",
    "    assert test_df.shape == (100, 5)\n",
    "# Display the data\n",
    "movies_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to be able to get somewhat of a combined plot between the two plots (imdb and wikipedia) which will give us a more objective plot description, parsing out the tonal words that are present in both and leaving behind only the most pertinent information.\n",
    "\n",
    "First, that means we have to create an overall combined item. We can do this by creating a new column that contains both descriptions in their raw state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine wiki_plot and imdb_plot into a single column\n",
    "movies_df[\"plot\"] = movies_df[\"wiki_plot\"].astype(str) + \"\\n\" + movies_df[\"imdb_plot\"].astype(str)\n",
    "\n",
    "# Inspect the new DataFrame\n",
    "movies_df.head()\n",
    "\n",
    "# Check that the shape has changed\n",
    "def test_shape2():\n",
    "    assert movies_df.shape == (100, 6)\n",
    "# Check that the new column is named properly\n",
    "def test_has_plot():\n",
    "    assert \"plot\" in movies_df.columns\n",
    "\n",
    "ipytest.run('-vv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, now that we have the combined description we can start to use the nltk package to tokenize the descriptions, separating them into the individual words and phrases like we wanted. The `nltk.tokenize` along with the regex package will allow us to parse out the words and parts separated by spaces and punctuation.\n",
    "\n",
    "Additionally, we can import the `nltk.stem.snowball` package so that we can create a stemming object. The stemming object should be set to English in our case because we are using an English dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Define a function to perform both stemming and tokenization\n",
    "def tokenize_and_stem(text):\n",
    "\n",
    "    # Tokenize by sentence, then by word\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "    # Filter out raw tokens to remove noise\n",
    "    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
    "\n",
    "    # Stem the filtered_tokens\n",
    "    stems = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "    return stems\n",
    "\n",
    "# Removes all of what it determines as the excess or noise from the text, keeps only the very root of the words and returns them in a list\n",
    "words_stemmed = tokenize_and_stem(\"It's 1941, and newspaper tycoon Charles Foster Kane (Orson Welles, who also directed and co-wrote the script) is dead\")\n",
    "\n",
    "print(words_stemmed)\n",
    "\n",
    "def test_stemming():\n",
    "    assert words_stemmed == ['it', \"'s\", 'and', 'newspap', 'tycoon', 'charl', 'foster', 'kane', 'orson', 'well', 'who', 'also', 'direct', 'and', 'co-wrot', 'the', 'script', 'is', 'dead']\n",
    "\n",
    "ipytest.run('-vv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will want to add weight to certain words based on frequency and uniqueness across documents. To do this we can use the Term Frequency - Inverse Document Frequency Vectorizer. What that means is that it will check the frequency of a term, increasing the weight but then reduce the weight of the term if it is found in several documents. This will allow us to find the most key terms like character names and thematic language as well as overlook the fluff words like \"the\" and \"and\" and other general structural words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate TfidfVectorizer object with stopwords and tokenizer\n",
    "# parameters for efficient processing of text\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem,\n",
    "                                 ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Once we establish a TF-IDF Vectorizer, we must modify the text to fit into the corresponding numeric form of the data which the computer will be able to understand and derive meaning from. To do this, we can call the <code>fit_transform()</code> method of the <code>TfidfVectorizer</code> object. </p>\n",
    "<p>In doing this process, we also set the <code>stop_words</code> parameter to 'english' so that the words that are dropped in the evaluation of the weight of the words are the words included in a set list of words in the nltk module.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform([x for x in movies_df[\"plot\"]])\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "def test_tfidf():\n",
    "    assert tfidf_matrix.shape[0] == (100)\n",
    "\n",
    "ipytest.run('-vv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To determine how closely one movie is related to the other by the help of unsupervised learning, we can use clustering techniques to group the movies into groups based on similarities. In our case, we will use the movie genre to form our clusters.</p>\n",
    "\n",
    "<p>We will use K-means, an algorithm which helps us to implement clustering in Python, which splits the set into a \"K\" amount of clusters based on the mean values.</p>\n",
    "\n",
    "<p>To combine with this, we will be using the cosine similarity angle to determine the closeness of relationship between two movies. To do this we calculate the similarity distance by using the formula <code>similarity_distance = 1 - cosine_similarity_angle</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import k-means to perform clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Import cosine_similarity to calculate similarity of movie plots\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create a KMeans object with 5 clusters and save as km\n",
    "km = KMeans(n_clusters=5)\n",
    "\n",
    "# Fit the k-means object with tfidf_matrix\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# Create a column cluster to denote the generated cluster for each movie\n",
    "movies_df[\"cluster\"] = clusters\n",
    "\n",
    "# Display number of films per cluster (clusters from 0 to 4)\n",
    "movies_df['cluster'].value_counts()\n",
    "\n",
    "similarity_distance = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we have the similarities calculated properly, we can use pyplot and scipy to create our dendrogram using the cluster and similarity distances to group the movies and display them appropriately distanced from each other.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure matplotlib to display the output inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Import modules necessary to plot dendrogram\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "mergings = linkage(similarity_distance, method='complete')\n",
    "\n",
    "dendrogram = dendrogram(mergings, labels=[x for x in movies_df[\"title\"]], leaf_rotation=90)\n",
    "\n",
    "fig = plt.gcf()\n",
    "_ = [lbl.set_color('r') for lbl in plt.gca().get_xmajorticklabels()]\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
